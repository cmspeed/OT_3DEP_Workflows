{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turkish-orchestra",
   "metadata": {
    "VoilaConfiguration": {
     "show_tracebacks": true
    }
   },
   "source": [
    "# Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS Hydrologic Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3b420",
   "metadata": {},
   "source": [
    "<h2>Table of Contents<span class=\"tocSkip\"></span></h2>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Authors\" data-toc-modified-id=\"Authors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Authors</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Funding\" data-toc-modified-id=\"Funding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Funding</a></span></li><li><span><a href=\"#Keywords\" data-toc-modified-id=\"Keywords-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Keywords</a></span></li><li><span><a href=\"#Citation\" data-toc-modified-id=\"Citation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Citation</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Installation-Options\" data-toc-modified-id=\"Installation-Options-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Installation Options</a></span><ul class=\"toc-item\"><li><span><a href=\"#Option-1:-Install-and-run-on-Google-Colaboratory\" data-toc-modified-id=\"Option-1:-Install-and-run-on-Google-Colaboratory-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Option 1: Install and run on Google Colaboratory</a></span></li><li><span><a href=\"#Option-2:-Install-and-run-on-local-file-system\" data-toc-modified-id=\"Option-2:-Install-and-run-on-local-file-system-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Option 2: Install and run on local file system</a></span></ul></li><li><span><a href=\"#Library-Imports\" data-toc-modified-id=\"Library-Imports-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Library Imports</a></span></li><li><span><a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Define Functions</a></span></ul></li><li><span><a href=\"#Data-Access-and-Processing\" data-toc-modified-id=\"Data-Access-and-Processing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Access and Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-3DEP-Dataset-Boundary-Polygons\" data-toc-modified-id=\"Get-3DEP-Dataset-Boundary-Polygons-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Get 3DEP Dataset Boundary Polygons</a></span></li><li><span><a href=\"#Create-Interactive-Ipyleaflet-Map-and-Define-the-Location-of-the-Watershed-for-3DEP-Query\" data-toc-modified-id=\"Create-Interactive-Ipyleaflet-Map-and-Define-the-Location-of-the-Watershed-for-3DEP-Query-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span> Create Interactive Ipyleaflet Map and Define the Location of the Watershed for 3DEP Query</a></span></li><li><span><a href=\"#Query-the-USGS-Watershed-Boundary-Service\" data-toc-modified-id=\"Query-the-USGS-Watershed-Boundary-Service-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Query the USGS Watershed Boundary Service</a></span></li><li><span><a href=\"#Find-3DEP-Polygon(s)-Intersecting-AOI\" data-toc-modified-id=\"Find-3DEP-Polygon(s)-Intersecting-AOI-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Find 3DEP Polygon(s) Intersecting AOI</a></span></li><li><span><a href=\"#Specify-Point-Cloud-Resolution\" data-toc-modified-id=\"Specify-Point-Cloud-Resolution-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Specify Point Cloud Resolution</a></span></li><li><span><a href=\"#Construct-and-Exectute-PDAL-Pipeline-for-Point-Cloud-Data\" data-toc-modified-id=\"Construct-and-Exectute-PDAL-Pipeline-for-Point-Cloud-Data-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Construct and Exectute PDAL Pipeline for Point Cloud Data</a></span></li><li><span><a href=\"#Digital-Elevation-Model-(DEM)-Generation\" data-toc-modified-id=\"Digital-Elevation-Model-(DEM)-Generation-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Digital Elevation Model (DEM) Generation</a></span></li><ul class=\"toc-item\"><li><span><a href=\"#Make-Digital-Surface-Model-(DSM)\" data-toc-modified-id=\"Make-Digital-Surface-Model-(DSM)-7.7.1\"><span class=\"toc-item-num\">7.7.1&nbsp;&nbsp;</span>Make Digital Surface Model (DSM)</a></span></li><li><span><a href=\"#Make-Digital-Terrain-Model-(DTM)\" data-toc-modified-id=\"Make-Digital-Terrain-Model-(DTM)-7.7.2\"><span class=\"toc-item-num\">7.7.2&nbsp;&nbsp;</span>Make Digital Terrain Model (DTM)</a></span></ul></ul></li><li><span><a href=\"#Data-Visualization\" data-toc-modified-id=\"Data-Visualization-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Data Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-DEMs-(DSM/DTM)\" data-toc-modified-id=\"Visualize-DEMs-(DSM/DTM)-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Visualize DEMs (DSM/DTM)</a></span></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Resources</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632d18",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "-  Author1 = {\"name\": \"Cole Speed\", \"affiliation\": \"UNAVCO, Inc.\", \"email\": \"cole.speed@beg.utexas.edu\", \"orcid\": \"https://orcid.org/0000-0003-0166-3642\"}\n",
    "-  Author2 = {\"name\": \"Matthew Beckley\", \"affiliation\": \"UNAVCO, Inc.\", \"email\": \"beckley@unavco.org\", \"orcid\":\"\"}\n",
    "-  Author3 = {\"name\": \"Christopher Crosby\", \"affiliation\": \"UNAVCO, Inc.\", \"email\": \"crosby@unavco.org\", \"orcid\": \"https://orcid.org/0000-0003-2522-4193\"}\n",
    "-  Author4 = {\"name\": \"Viswanath Nandigam\", \"affiliation\": \"San Diego Supercomputer Center, UC San Diego\", \"email\": \"viswanat@sdsc.edu\", \"orcid\": \"https://orcid.org/0000-0003-0928-9851\"}\n",
    "    \n",
    "## Purpose\n",
    "\n",
    "The United States Geological Survey's 3-D Elevation Program (3DEP) is collecting high-quality light detection and ranging (lidar) data over the conterminous U.S., Hawaii, and the U.S territories. Data acquisition is ongoing, with over 1800 3DEP datasets consisting of more than 42 trillion points covering an area of greater than 6.5 million $km^{2}$ already available for use. Data are hosted and publically available in Entwine Point Tile (EPT) format in an<a href=\"https://registry.opendata.aws/usgs-lidar/\"> Amazon Web Services (AWS) S3 public bucket</a>.\n",
    "\n",
    "3DEP point cloud data can be accessed through several user interfaces including the <a href=\"https://portal.opentopography.org/datasets\">OpenTopography Data Portal</a> and the <a href=\"https://prd-tnm.s3.amazonaws.com/LidarExplorer/index.html#/process\">USGS Lidar Explorer</a>. In many cases, however, users may prefer to access and produce derivative products from the 3DEP lidar data programmatically, particularly in the case of more advanced users seeking to access and process immense swaths of lidar point cloud data on local, commercial cloud, or high performance compute infrastructure. However, to date tools and/or workflows for programmatic access and processing of the 3DEP data remain underdeveloped.\n",
    "\n",
    "This Jupyter Notebook is part of a suite developed by OpenTopography in collaboration with the USGS National Geospatial Program, in an effort to leverage available APIs and cloud resources to enhance access and usability of 3DEP data and products for the geospatial community. Importantly, these notebooks are designed to be accessible by user's with limited experience in the Python programming language and as educational tools for those looking to learn about accessing, processing, and visualizing cloud-hosted data with Python. These notebooks use open-source Python libraries, including the Point Data Abstraction Libary (PDAL) and Geospatial Data Abstraction Library (GDAL) for all point cloud and raster processing. \n",
    "\n",
    "This and the other use-case specific Jupyter Notebooks developed as part of this effort for programmatic 3DEP access are available in a <a href=\"https://github.com/cmspeed/OT_3DEP_Workflows\"> Github repository </a> and may be run locally or on the <a href=\"https://colab.research.google.com/\">Google Colaboratory</a> cloud platform.\n",
    "\n",
    "#### Specific features of this notebook\n",
    "\n",
    "1. User selects a point location on an interactive map and the corresponding USGS 12-digit (Sub-watershed) or 14-digit hydrologic unit (HU) is requested via the <a href= \"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/\"> USGS Watershed Boundary Dataset Map Service </a>.\n",
    "\n",
    "2. API request to <a href=\"https://registry.opendata.aws/usgs-lidar/\"> Amazon Web Services (AWS) EPT (Entwine Point Tile) S3 bucket</a> returns 3DEP point cloud data within user-defined 12- or 14-digit hydrologic unit (HU) for subsequent processing. \n",
    "\n",
    "3. Flexible and customizable PDAL pipelines are available for specifying point cloud resolution, filtering, reclassifying, and reprojecting points.\n",
    "\n",
    "4. Option to create a Digital Terrain Model (DTM) or Digital Surface Model (DSM) for the hydrologic unit with user-specifed resolution, gridding method, and file type.\n",
    "\n",
    "5. Point cloud data and/or derivative products are saved to local directory or on Google Drive (if user uses Google Colaboratory option, see below.)\n",
    "    \n",
    "## Funding\n",
    "\n",
    "Funding for the creation and distribution of these Jupyter Notebook-based workflows was provided as by the <a href=\"https://www.usgs.gov/centers/community-for-data-integration-cdi\">USGS Community for Data Integration (CDI)</a> through the funded grant <a href=\"https://www.usgs.gov/centers/community-for-data-integration-cdi/science/enhancing-usability-3dep-data-and-web-services\"> *Enhancing usability of 3DEP data and web services with Jupyter notebooks*</a>. OpenTopography is supported by the National Science Foundation (NSF) under Award Numbers <a href=\"https://nsf.gov/awardsearch/showAward?AWD_ID=1948997\">1948997</a>, <a href=\"https://nsf.gov/awardsearch/showAward?AWD_ID=1948994\">1948994</a> & <a href =\"https://nsf.gov/awardsearch/showAward?AWD_ID=1948857\">1948857</a>.\n",
    "\n",
    "## Keywords\n",
    "\n",
    "keywords=[\"OpenTopography\",\"USGS\", \"CDI\", \"3DEP\", \"PDAL\", \"Watersheds\"]\n",
    "\n",
    "## Citation\n",
    "To cite this notebook:  Speed, C., Beckley, M., Crosby, C., & Nandigam, V. (2022). # Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS Hydrologic Units (Version v1.0). DOI: Accessed: MM/DD/YYYY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83dbc2a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e20850",
   "metadata": {},
   "source": [
    "### Installation Options\n",
    "There are two options for performing the workflow steps outlined below. **Option 1** is our suggested method for simplicity, as building a virtual environment with the required dependencies on the user's local file system can be challenging based on the user's experience level with Python and <a href=\"https://www.anaconda.com/\"> Anaconda</a>.\n",
    "\n",
    "1. **Option 1**: Launch the interactive Jupyter notebook on Google Colaboratory.\n",
    "    - Does not require creation of a virtual environment or installation on local file system.\n",
    "    - Requires Google account and access to personal Google Drive folder.\n",
    "    - Data download limits will be dependent on user's available Google Drive storage. \n",
    "    - If you wish to run this notebook in Google Colaboratory click the 'Open in Colab' badge below. \n",
    "    <br/><br/>\n",
    "2. **Option 2**: Download this Jupyter notebook (.ipynb file) to your local file system.\n",
    "    - Create a virtual conda environment containing the required dependencies.\n",
    "    - Run Juypter notebook on local machine.\n",
    "    - Data download limits and computation speed will be dependent on user's hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa4314",
   "metadata": {},
   "source": [
    "### Option 1: Install and run on Google Colaboratory\n",
    "For ease-of-use, it is suggested to launch and execute these notebooks on <a href=\"https://colab.research.google.com/\">Google Colaboratory</a> (Colab, for short), Google's Cloud Platform. Dependencies will be installed on a virtual machine on Google's cloud servers and the code will be executed directly in your browser! A major benefit of this is that you will have direct access to Google's high-end CPU/GPUs and will not have to install any dependencies locally. All deliverables will be saved to your personal Google Drive. To experiment and run one of the below Jupyter Notebooks on Google Colab click the \"Open in Colab\" badge below.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/03_3DEP_Generate_DEM_USGS_HUCs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf94aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell only excecutes if you're running on Colab. Installation process takes 2-3 minutes.\n",
    "import os, sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "  # Colab requires proj_lib environment variable to be set manually.\n",
    "  os.environ['PROJ_LIB'] = '/usr/local/share/proj/'\n",
    "    \n",
    "  # Mount Google Drive. You will be prompted to grant file I/O access to Drive.\n",
    "  from google.colab import drive \n",
    "  drive.mount('/gdrive/') # Mount Google Drive! \n",
    "\n",
    "  # Clone OpenTopography 3DEP Workflow Git Repository\n",
    "  !git clone https://github.com/cmspeed/OT_3DEP_Workflows\n",
    "\n",
    "  # Install Conda (necessary to install PDAL/GDAL)\n",
    "  !pip install -q condacolab\n",
    "  import condacolab\n",
    "  condacolab.install()\n",
    "\n",
    "  #kernel will restart. Install PDAL and GDAL\n",
    "  !mamba install -q python-pdal gdal\n",
    "\n",
    "  #  Install the remaining dependencies from requirements.txt\n",
    "  !pip install -r OT_3DEP_Workflows/requirements.txt\n",
    "  \n",
    "  #restart runtime\n",
    "  exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cb286",
   "metadata": {},
   "source": [
    "**Runtime must be restarted before import of PDAL will be successful!**\n",
    "\n",
    "- On Google Colab ribbon click \"Runtime\" > \"Restart Runtime\".\n",
    "- Do not execute any of the previous cells. Execute the following cells only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98017c0f",
   "metadata": {},
   "source": [
    "**If using Option 1 (Google Colab), proceed to Library Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ca7ad",
   "metadata": {},
   "source": [
    "### Option 2: Install and run on local file system\n",
    "\n",
    "If you would like to run the Jupyter Notebook on your local machine:\n",
    "\n",
    "Make a new directory on your local file system where the 3DEP Jupyter Notebooks (and all 3DEP data, if desired) will be saved. In this example case, the directory will be called `3DEP`.\n",
    "  \n",
    "    $ mkdir 3DEP\n",
    "\n",
    "Change into the new directory and `git clone` the Github repository containing the Jupyter Notebooks and other relevant files to your local file system.\n",
    "\n",
    "    $ cd 3DEP\n",
    "$ git clone https://github.com/cmspeed/OT_3DEP_Workflows\n",
    "\n",
    "Anaconda is recommended for Python package installation and management. Package versions in Anaconda are managed by the package management system *conda*. Anaconda installers for MacOS/Linux/Windows can be downloaded from https://docs.anaconda.com/anaconda/install/. Follow the instructions to install Anaconda.\n",
    "\n",
    "After installing Anaconda, create a virtual environment with the required dependencies, using the `environment.yml` file contained in the cloned Github repo). Note: Exectuting the following command will automatically create the conda environment with name `3dep` and all of the required dependencies installed. If you would prefer a different name, replace `3dep` with another name in the following command:\n",
    "\n",
    "\t$ conda env create -n 3dep --file environment.yml\n",
    "\n",
    "Next, activate the conda environment with all of the necessary dependencies installed. \n",
    "\t\n",
    "\t$ conda activate 3dep\n",
    "    \n",
    "To use the Ipython kernel in this newly created conda environement, it will need to be installed manually.\n",
    "\n",
    "After activating the newly created environment (in this case called '3dep'), install the IPython kernel for Jupyter:\n",
    "\n",
    "    $ pip install --user ipykernel\n",
    "    \n",
    "Next, add the virtual environement you just created to Jupyter (assuming that the conda environement is called '3dep'. If not replace '3dep' with the name of your newly created environement in the command below:\n",
    "\n",
    "    $ python -m ipykernel install --user --name=3dep\n",
    "\n",
    "Now, launch the chosen Jupyter Notebook. If unsure how to launch a Notebook, refer to this guide (https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html). \n",
    "\n",
    "Once you are viewing the notebook, on the upper toolbar click \"Kernel\" > \"Change Kernel\" > \"3dep\" (or the name of your newly created conda environment.\n",
    "\n",
    "**You may now proceed to Library Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-vampire",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "\n",
    "After successfully completing the steps outlined in either **Option 1** or **Option 2**, we can now import the modules for use throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce19509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import copy\n",
    "import geopandas as gpd\n",
    "import ipyleaflet\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import pdal\n",
    "import pyproj\n",
    "import requests\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "from shapely.ops import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1299e5e",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "Several functions are provided in the cell below. These functions are necessary for successful execution of remainder of the notebook. Broadly, these functions provide the utility for the user to draw and area of interest (AOI) on an interactive map and construct the PDAL pipeline for getting the point cloud data from the Amazon Web Services EPT bucket, performing processing steps, producing DEMs, and saving the results. A description of the parameters are provided as docstrings in the function definitions.\n",
    "\n",
    "**These functions can be modified as the user sees fit; however, they are designed to work with a simple execution of the below cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d924c977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def proj_to_3857(poly, orig_crs):\n",
    "    \"\"\"\n",
    "    Function for reprojecting a polygon from a shapefile of any CRS to Web Mercator (EPSG: 3857).\n",
    "    The original polygon must have a CRS assigned.\n",
    "    \n",
    "    :param poly: shapely polygon for user area of interest (AOI)\n",
    "    :param orig_crs: the original CRS for the shapefile. It is stripped out during import_shapefile_to_shapely() method\n",
    "    \"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project_gcs = pyproj.Transformer.from_crs(orig_crs, wgs84, always_xy=True).transform\n",
    "    project_wm = pyproj.Transformer.from_crs(orig_crs, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj4326 = transform(project_gcs, poly)\n",
    "    user_poly_proj3857 = transform(project_wm, poly)\n",
    "    return(user_poly_proj4326, user_poly_proj3857)\n",
    "\n",
    "def gcs_to_proj(poly):\n",
    "    \"\"\"\n",
    "    Function for reprojecting polygon shapely object from geographic coordinates (EPSG:4326) \n",
    "    to Web Mercator (EPSG: 3857)). \n",
    "    \n",
    "    :param poly: shapely polygon for user area of interest (AOI)\n",
    "    \"\"\"\n",
    "    wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "    web_mercator = pyproj.CRS(\"EPSG:3857\")\n",
    "    project = pyproj.Transformer.from_crs(wgs84, web_mercator, always_xy=True).transform\n",
    "    user_poly_proj3857 = transform(project, poly)\n",
    "    return(user_poly_proj3857)\n",
    "\n",
    "def import_shapefile_to_shapely(path):\n",
    "    \"\"\"\n",
    "    Conversion of shapefile to shapely object.\n",
    "    \n",
    "    :param path: location of shapefile on user's local file system\n",
    "    \"\"\"\n",
    "    shapefile_path = path\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    orig_crs = gdf.crs                   # this is the original CRS of the imported shapefile\n",
    "    user_shp = gdf.loc[0, 'geometry']\n",
    "    user_shp_epsg4326, user_shp_epsg3857 = proj_to_3857(user_shp, orig_crs)\n",
    "    user_AOI = [[user_shp_epsg4326, user_shp_epsg3857]]\n",
    "    return user_AOI\n",
    "    \n",
    "def handle_draw(target, action, geo_json):\n",
    "    \"\"\"\n",
    "    Functionality to draw area of interest (AOI) on interactive ipyleaflet map.\n",
    "    \n",
    "    :param extent_epsg3857: polygon for user-defined AOI\n",
    "    :param usgs_3dep_dataset_name: name of 3DEP dataset which AOI overlaps\n",
    "    :param resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "    \"\"\"\n",
    "        \n",
    "    geom = dict(geo_json['geometry'])\n",
    "    user_poly = shape(geom)\n",
    "    user_poly_proj3857 = gcs_to_proj(user_poly)\n",
    "    print('AOI is valid and has boundaries of ', user_poly_proj3857.bounds, 'Please proceed to the next cell.')\n",
    "    user_AOI.append((user_poly, user_poly_proj3857))  #for various reasons, we need user AOI in GCS and EPSG 3857\n",
    "    \n",
    "\n",
    "def build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_names, pc_resolution, filterNoise = False,\n",
    "                        reclassify = False, savePointCloud = True, outCRS = 3857, pc_outName = 'filter_test', \n",
    "                        pc_outType = 'laz'):\n",
    "\n",
    "    \"\"\"\n",
    "    Build pdal pipeline for requesting, processing, and saving point cloud data. Each processing step is a 'stage' \n",
    "    in the final pdal pipeline. Each stages is appended to the 'pointcloud_pipeline' object to produce the final pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    :param extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857)Polygon is generated \n",
    "                            either through the 'handle_draw' methor or by inputing their own shapefile.\n",
    "    :param usgs_3dep_dataset_names: List of name of the 3DEP dataset(s) that the data will be obtained. This parameter is set \n",
    "                                determined through intersecttino of the 3DEP and AOI polys.\n",
    "    :param pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "        \n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "                            \n",
    "    :param filterNoise: Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "    :param savePointCloud: Option to save (or not) the point cloud data. If savePointCloud == False, \n",
    "           the pc_outName and pc_outType parameters are not used and can be any value.\n",
    "    :param outCRS: Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "    :param pc_outName: Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    :param pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                  pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    :raise Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #this is the basic pipeline which only accesses the 3DEP data\n",
    "    readers = []\n",
    "    for name in usgs_3dep_dataset_names:\n",
    "        url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/{}/ept.json\".format(name)\n",
    "        reader = {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": str(url),\n",
    "            \"polygon\": str(extent_epsg3857),\n",
    "            \"requests\": 3,\n",
    "            \"resolution\": pc_resolution\n",
    "        }\n",
    "        readers.append(reader)\n",
    "        \n",
    "    pointcloud_pipeline = {\n",
    "            \"pipeline\":\n",
    "                readers\n",
    "    }\n",
    "    \n",
    "    if filterNoise == True:\n",
    "        \n",
    "        filter_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification![7:7], Classification![18:18]\"\n",
    "        }\n",
    "        \n",
    "        pointcloud_pipeline['pipeline'].append(filter_stage)\n",
    "    \n",
    "    if reclassify == True:\n",
    "        \n",
    "        remove_classes_stage = {\n",
    "            \"type\":\"filters.assign\",\n",
    "            \"value\":\"Classification = 0\"\n",
    "        }\n",
    "        \n",
    "        classify_ground_stage = {\n",
    "            \"type\":\"filters.smrf\"\n",
    "        }\n",
    "        \n",
    "        reclass_stage = {\n",
    "            \"type\":\"filters.range\",\n",
    "            \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "    \n",
    "        pointcloud_pipeline['pipeline'].append(remove_classes_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(classify_ground_stage)\n",
    "        pointcloud_pipeline['pipeline'].append(reclass_stage)\n",
    "    \n",
    "    reprojection_stage = {\n",
    "        \"type\":\"filters.reprojection\",\n",
    "        \"out_srs\":\"EPSG:{}\".format(outCRS)\n",
    "    }\n",
    "    \n",
    "    pointcloud_pipeline['pipeline'].append(reprojection_stage)\n",
    "    \n",
    "    if savePointCloud == True:\n",
    "        \n",
    "        if pc_outType == 'las':\n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        elif pc_outType == 'laz':    \n",
    "            savePC_stage = {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"compression\": \"laszip\",\n",
    "                \"filename\": str(pc_outName)+'.'+ str(pc_outType),\n",
    "            }\n",
    "        else:\n",
    "            raise Exception(\"pc_outType must be 'las' or 'laz'.\")\n",
    "\n",
    "        pointcloud_pipeline['pipeline'].append(savePC_stage)\n",
    "        \n",
    "    return pointcloud_pipeline\n",
    "\n",
    "\n",
    "def make_DEM_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution, dem_resolution,\n",
    "                      filterNoise = True, reclassify = True, savePointCloud = False, outCRS = 3857,\n",
    "                      pc_outName = 'filter_test', pc_outType = 'laz', demType = 'dtm', gridMethod = 'idw', \n",
    "                      dem_outName = 'dem_test', dem_outExt = 'tif', driver = \"GTiff\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build pdal pipeline for creating a digital elevation model (DEM) product from the requested point cloud data. The \n",
    "    user must specify whether a digital terrain (bare earth) model (DTM) or digital surface model (DSM) will be created, \n",
    "    the output DTM/DSM resolution, and the gridding method desired. \n",
    "\n",
    "    The `build_pdal_pipeline() method is used to request the data from the Amazon Web Services ept bucket, and the \n",
    "    user may define any processing steps (filtering, reclassifying, reprojecting). The user must also specify whether \n",
    "    the point cloud should be saved or not. Saving the point cloud is not necessary for the generation of the DEM. \n",
    "\n",
    "    \n",
    "    :param extent_epsg3857: Polygon for user-defined AOI in Web Mercator projection (EPS:3857). Polygon is generated \n",
    "                         either through the 'handle_draw' methor or by inputing their own shapefile.\n",
    "                         This parameter is set automatically when the user-defined AOI is chosen.\n",
    "    :param usgs_3dep_dataset_names: List of name of the 3DEP dataset(s) that the data will be obtained. This parameter is set \n",
    "                                determined through intersecttino of the 3DEP and AOI polys.\n",
    "    :param pc_resolution: The desired resolution of the pointcloud based on the following definition:\n",
    "\n",
    "                        Source: https://pdal.io/stages/readers.ept.html#readers-ept\n",
    "                            A point resolution limit to select, expressed as a grid cell edge length. \n",
    "                            Units correspond to resource coordinate system units. For example, \n",
    "                            for a coordinate system expressed in meters, a resolution value of 0.1 \n",
    "                            will select points up to a ground resolution of 100 points per square meter.\n",
    "                            The resulting resolution may not be exactly this value: the minimum possible \n",
    "                            resolution that is at least as precise as the requested resolution will be selected. \n",
    "                            Therefore the result may be a bit more precise than requested.\n",
    "\n",
    "\n",
    "   \n",
    "    :param pc_outName: Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    :param pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If savePointcloud = False, \n",
    "                  pc_outName can be in value. If a different file type is requested,the user will get error.\n",
    "    :raise Exception: If user passes in argument that is not 'las' or 'laz'.\n",
    "    \n",
    "    :param dem_resolution: Desired grid size (in meters) for output raster DEM \n",
    "    :param filterNoise: Option to remove points from USGS Class 7 (Low Noise) and Class 18 (High Noise).\n",
    "    :param savePointCloud: Option to save (or not) the point cloud data. If savePointCloud == False, \n",
    "           the pc_outName and pc_outType parameters are not used and can be any value.\n",
    "\n",
    "    :param outCRS: Output coordinate reference systemt (CRS), specified by ESPG code (e.g., 3857 - Web Mercator)\n",
    "    :param pc_outName: Desired name of file on user's local file system. If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    :param pc_outType:  Desired file extension. Input must be either 'las' or 'laz'. If a different file type is requested,\n",
    "                  the user will get error stating \"Extension must be 'las' or 'laz'\". If savePointcloud = False, \n",
    "                  pc_outName can be in value.\n",
    "    :param demType: Type of DEM produced. Input must 'dtm' (digital terrain model) or 'dsm' (digital surface model).\n",
    "    :param gridMethod: Method used. Options are 'min', 'mean', 'max', 'idw'.\n",
    "    :param dem_outName: Desired name of DEM file on user's local file system.\n",
    "    :param dem_outExt: DEM file extension. Default is TIF.\n",
    "    :param driver: File format. Default is GTIFF\n",
    "    :raise Exception: If user passes in argument that is not 'dtm' or 'dsm'\n",
    "    \"\"\"\n",
    "\n",
    "    dem_pipeline = build_pdal_pipeline(extent_epsg3857, usgs_3dep_dataset_name, pc_resolution,\n",
    "                                              filterNoise, reclassify, savePointCloud, outCRS, pc_outName, pc_outType)\n",
    "    \n",
    "    if demType == 'dsm':\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    elif demType == 'dtm':\n",
    "        groundfilter_stage = {\n",
    "                \"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"\n",
    "        }\n",
    "\n",
    "        dem_pipeline['pipeline'].append(groundfilter_stage)\n",
    "\n",
    "        dem_stage = {\n",
    "                \"type\":\"writers.gdal\",\n",
    "                \"filename\":str(dem_outName)+ '.' + str(dem_outExt),\n",
    "                \"gdaldriver\":driver,\n",
    "                \"nodata\":-9999,\n",
    "                \"output_type\":gridMethod,\n",
    "                \"resolution\":float(dem_resolution),\n",
    "                \"gdalopts\":\"COMPRESS=LZW,TILED=YES,blockxsize=256,blockysize=256,COPY_SRC_OVERVIEWS=YES\"\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"demType must be 'dsm' or 'dtm'.\")\n",
    "        \n",
    "        \n",
    "    dem_pipeline['pipeline'].append(dem_stage)\n",
    "    \n",
    "    return dem_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433ad51",
   "metadata": {},
   "source": [
    "## Data Access and Processing\n",
    "Now that we have the required modules imported and functions defined, we can proceed with defining our area of interest (AOI), accessing/processing the 3DEP data from the Amazon Web Services EPT bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecbeb2",
   "metadata": {},
   "source": [
    "### Get 3DEP Dataset Boundary Polygons  \n",
    "First, we will get the 3DEP dataset polygon boundaries to see if 3DEP data is currently available for our area of interest. An up-to-date version of the currently available 3DEP dataset boundaries are maintained by Hobu Inc. in the usgs-lidar Github repository https://github.com/hobuinc/usgs-lidar/. \n",
    "\n",
    "In the following cell, we use an API request to get the boundaries from the repository and save a local copy of these boundaries in geojson format. Next, we create a geopandas dataframe object to easily access the names, url, and point count of each dataset, and we use the ```gcs_to_proj()``` function to project each 3DEP polygon geometry to Web Mercator projection (EPSG: 3857), which is the native projection of the 3DEP data in the AWS S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8debe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting, loading, and projecting 3DEP dataset polygons...\n",
      "Done. 3DEP polygons downloaded and projected to Web Mercator (EPSG:3857)\n"
     ]
    }
   ],
   "source": [
    "# Get GeoJSON file for 3DEP outlines from URL \n",
    "\n",
    "print(\"Requesting, loading, and projecting 3DEP dataset polygons...\")\n",
    "\n",
    "#request the boundaries from the Github repo and save locally.\n",
    "url = 'https://raw.githubusercontent.com/hobuinc/usgs-lidar/master/boundaries/resources.geojson'\n",
    "r = requests.get(url)\n",
    "with open('resources.geojson', 'w') as f:\n",
    "    f.write(r.content.decode(\"utf-8\"))\n",
    "\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    geojsons_3DEP = json.load(f)\n",
    "    \n",
    "#make pandas dataframe and create pandas.Series objects for the names, urls, and number of points for each boundary.\n",
    "with open('resources.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "    names = df['name']\n",
    "    urls = df['url']\n",
    "    num_points = df['count']\n",
    "\n",
    "#project the boundaries to EPSG 3857 (necessary for API call to AWS for 3DEP data)\n",
    "projected_geoms = []\n",
    "for geometry in df['geometry']:\n",
    "        projected_geoms.append(gcs_to_proj(geometry))\n",
    "\n",
    "geometries_GCS = df['geometry']\n",
    "geometries_EPSG3857 = gpd.GeoSeries(projected_geoms)\n",
    "\n",
    "print('Done. 3DEP polygons downloaded and projected to Web Mercator (EPSG:3857)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d0f8f",
   "metadata": {},
   "source": [
    "###  Create Interactive Ipyleaflet Map and Define the Location of the Watershed for 3DEP Query\n",
    "Next, we will specify the location of the desired USGS 12-digit (Subwatershed) or 14-digit HUC using an ipyleaflet interactive map and the 3DEP dataset polygons loaded in the prior step.  There are currently two options for specifying the watershed:\n",
    "\n",
    "1. **Option 1** - Define a point (lat/lon) on the map that you would like the watershed boundary. If you know the coordinates of a point within the watershed, replace the values for `lon` and `lat` below.\n",
    "\n",
    "2. **Option 2** - User places a maker on an ipyleaflet interactive map to define location. If you don't know the coordinates, execute the following cell, leaving lon/lat = 0.0.\n",
    "\n",
    "Running the next cell will produce an interactive map, make an ipyleaflet layer out of the 3DEP dataset polygon geojson, and allow the user to place a marker to define the location of the desired USGS 12- or 14-digit HUC. When the map is rendered, use the +/- symbols to zoom in and out. The currently available 3DEP datasets are shown in transparent green polygons. The current functionality of the notebook allows the user AOI to straddle multiple 3DEP datsets. In this case, points from both datasets will be queried and merged into the resulting las/laz file.\n",
    "\n",
    "**Important Note 1: Use the 'marker' shaped button on the left sidebar to place a marker on the map. None of the other tools in the sidebar should be used in this notebook.**\n",
    "\n",
    "**Important Note 2: If the user would like to place a new marker, the cell below must be re-executed each and every time. If multiple markers are drawn on the same interactive map without re-executing the cell, subsequent cells will not function appropriately. The cell must be run each and every time the user would like to define a new AOI.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33dc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace lon/lat to use option 1, leave both as 0.0 to use option 2.\n",
    "# cell must be executed for both option 1 and option 2.\n",
    "lon = 0.0\n",
    "lat = 0.0\n",
    "location = Point(lon,lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413b5707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place a marker on the map to locate the appropriate watershed where you would like 3DEP data using the tools on the left side of the map.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c09d8c900f42179b2d64971259e96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[37, -100], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out_tâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI is valid and has boundaries of  (-10709012.826636981, 6100728.324644107, -10709012.826636981, 6100728.324644107) Please proceed to the next cell.\n"
     ]
    }
   ],
   "source": [
    "geo_json_3DEP = ipyleaflet.GeoJSON(data=geojsons_3DEP, style = {'color': 'green', 'opacity':1, \n",
    "                                       'weight':1, 'fillOpacity':0.1})\n",
    "\n",
    "dc = ipyleaflet.DrawControl(\n",
    "    marker={\"shapeOptions\": {\"color\": \"blue\"}},\n",
    "    polyline={},\n",
    "    polygon={},\n",
    "    circlemarker={},\n",
    ")\n",
    "\n",
    "if location.coords[0] != (0.0, 0.0):\n",
    "    user_AOI = [[location]]\n",
    "    marker = ipyleaflet.Marker(location=(lat,lon), draggable=False)\n",
    "\n",
    "    m = ipyleaflet.Map(\n",
    "        basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "        center=(lat, lon),\n",
    "        zoom=7,\n",
    "        crs=ipyleaflet.projections.EPSG3857\n",
    "        )\n",
    "    \n",
    "    m.add_layer(geo_json_3DEP)  \n",
    "    m.add_layer(marker);\n",
    "    display(m)\n",
    "    print(\"The blue marker indicates the location of the lat/lon provided. Proceed to the next cells to get the corresponding HUC boundary\")\n",
    "\n",
    "else:\n",
    "    print('Place a marker on the map to locate the appropriate watershed where you would like 3DEP data using the tools on the left side of the map.')\n",
    "    \n",
    "    m = ipyleaflet.Map(\n",
    "        basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "        center=(37, -100),\n",
    "        zoom=3.5,\n",
    "        crs=ipyleaflet.projections.EPSG3857\n",
    "        )\n",
    "    \n",
    "    m.add_layer(geo_json_3DEP)  #add 3DEP polygons GeoJSON\n",
    "    \n",
    "    user_AOI = []\n",
    "    dc.on_draw(handle_draw)\n",
    "    m.add_control(dc)\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f56b37",
   "metadata": {},
   "source": [
    "### Query the USGS Watershed Boundary Service\n",
    "Now that the user-defined location is specified, the following cell will make an API request to the USGS Map Service for the 12- or 14-digit HUC boundaries and return the relevant quadrangle as a geojson.\n",
    "\n",
    "The  `url = ../MapServer/6/` corresponds to the 12-digit HUCs. The  `url = ../MapServer/7/` corresponds to the 14-digit HUCs (commented out by default). The index for other map boundaries can be found <a href= \"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/\"> here </a>. Keep in mind that the 12-digit (subwatershed) HUs usually contain up to 1 billion points. Selecting anything larger (e.g., 10-digit (Watershed) HUs) will result in *very* large point clouds and much longer compute times. We have not tested functionality on HU's larger than 12-digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat bounding box for query\n",
    "user_AOI_center = user_AOI[-1][0].centroid\n",
    "user_AOI = user_AOI[-1][0].bounds\n",
    "user_AOI = str(user_AOI)\n",
    "user_AOI = user_AOI.strip('()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b50a18",
   "metadata": {},
   "source": [
    "Now exectute the call to the USGS Watershed Boundary Dataset REST API. There are several possibilities here if the following step does not execute properly. There is a print statement implemented that should provide some indication of which it is. \n",
    "\n",
    "1. If you receive the error `Error with Service Call` or `Error loading JSON output`, this most likely indicates that the region you selected does coincide with a USGS hydrologic unit from that particular service.\n",
    "\n",
    "2. If you recieve the error `{'message': 'Endpoint request timed out'}`, this likely indicates the Watershed Boundary Dataset service may be down. To check, click this link (https://stats.uptimerobot.com/gxzRZFARLZ/783928857). If you are greeted with the message `ScienceBase is down.`, the service is temporarily down and it is not possible to query the service at this time. \n",
    "\n",
    "If the cell executes successfully, the interesecting watershed boundary geometry will be printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for 12-digit HUCs is ../MapServer/6/\n",
    "#url = 'https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/7/query?'   #14-Digit HUC\n",
    "url = 'https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/6/query?'   #12-Digit HUC\n",
    "\n",
    "#the parameters here will query the map server for the appropriate HU boundary\n",
    "params = dict(geometry=user_AOI,geometryType='esriGeometryEnvelope',inSR='4326',\n",
    "              spatialRel='esriSpatialRelIntersects',f='geojson')\n",
    "\n",
    "#Execute REST API call.  \n",
    "try:\n",
    "    r = requests.get(url,params=params)\n",
    "except:\n",
    "    print('Error with Service Call. This could mean that there is no hydrologic unit polygon where you selected.')\n",
    "\n",
    "#load API JSON output into a variable\n",
    "try:\n",
    "    wbd_geojson = json.loads(r.content)\n",
    "    print(wbd_geojson)   \n",
    "except:\n",
    "    print('Error loading JSON output')\n",
    "\n",
    "#To write out a JSON file...\n",
    "with open('WBD_API_Query.geojson', 'w') as outfile:\n",
    "    json.dump(wbd_geojson, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffb498",
   "metadata": {},
   "source": [
    "### Find 3DEP Polygon(s) Intersecting AOI\n",
    "Now that the user-specified hydrologic unit boundary is defined, the following cell will determine the intersecting 3DEP dataset names and show them on an interactive map. These names will be used in the API request to the AWS EPT bucket along with the user-defined quad. `intersecting_polys` will be a list of the intersecting 3DEP dataset name(s), boundary(ies) in EPSG: 4326, boundary(ies) in EPSG: 3857, url(s), and the number of points in the entire 3DEP dataset(s). The dataset names will be used in the API request to the AWS EPT S3 bucket. A ratio of the total number of points and the area of the user-defined AOI will be used to estimate the total points within the AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WBD_API_Query.geojson\") as jsonFile:\n",
    "        wbd_geojson = json.load(jsonFile)\n",
    "        jsonData = wbd_geojson[\"features\"]\n",
    "        WBD_names = []\n",
    "        WBD_geometries = []\n",
    "        for x in jsonData:\n",
    "            for value in x['properties'].values():                \n",
    "                WBD_names.append(value)\n",
    "\n",
    "with open('WBD_API_Query.geojson', 'r') as f:\n",
    "    df = gpd.read_file(f)\n",
    "\n",
    "projected_geoms = []\n",
    "for geometry in df['geometry']:\n",
    "        projected_geoms.append(gcs_to_proj(geometry))\n",
    "\n",
    "user_wbd12 = df['geometry']\n",
    "user_wbd12_EPSG3857 = gpd.GeoSeries(projected_geoms)\n",
    "        \n",
    "    \n",
    "\n",
    "#here find the interesecting 3DEP datasets\n",
    "intersecting_polys = []\n",
    "for i,geom in enumerate(geometries_EPSG3857):\n",
    "    if geom.intersects(user_wbd12_EPSG3857[0]):\n",
    "        intersecting_polys.append((names[i], geometries_GCS[i], geometries_EPSG3857[i], urls[i], num_points[i]))\n",
    "        \n",
    "if len(intersecting_polys) == 0:\n",
    "    raise Exception('the watershed you selected or coordinates you entered do not overlap existing 3DEP data.')\n",
    "\n",
    "print(intersecting_polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3867e",
   "metadata": {},
   "source": [
    "### Specify Point Cloud Resolution\n",
    "Executing the next cell will show the AOI, the relevant 3DEP dataset(s) on another interactive map, and the option to specify point cloud resolution. \n",
    "\n",
    "Importantly, after the map is rendered, the user must define the desired 'point cloud resolution' using the radio buttons below the map. An estimation of the total number of lidar points within the bounding box is provided based on the area of the AOI and the total number of lidar points in the 3DEP dataset(s). Selecting the \"Full\" option will return all points in the quad (this number will be quite large, >1 billion points in some cases). Selecting any of the other options for resoltuion will return the appropriate number of points to ensure at least one lidar point per Nth meter (where N is the chosen resolution). The user is not alone if they are at least somewhat confused by the concept of resolution in this sense. The Entwine Point Tile (EPT) file format utilizes an octree structure for the point cloud, and in essense, the chosen resolution defines how deep in the octree to request points to obtain the specified resolution. This depth, and total number points varies drastically based on a number of parameters including local topography and vegetation. Therefore, the 'resolution' paramater and the total point count do not scale linearly. In other words, specifying a resolution of 2m will likely return far less than half of the number of points returned with 'full' resolution. The estimate of the full poin total provided is not exact, but should give the user some idea of how many points to expect the resultant point cloud to contain. \n",
    "\n",
    "Select the appropriate radio button below the map to specify `pointcloud_resolution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find AOI center for plotting purposes\n",
    "centroid =  list(AOI_GCS.centroid.coords)[0]\n",
    "\n",
    "#make ipyleaflet map\n",
    "m = ipyleaflet.Map(\n",
    "    basemap=ipyleaflet.basemaps.Esri.WorldTopoMap,\n",
    "    center=(centroid[1],centroid[0]),\n",
    "    zoom=12,\n",
    "    )\n",
    "\n",
    "#add intersecting 3DEP polygon(s) to the map\n",
    "wlayer_3DEP_list = []\n",
    "usgs_3dep_datasets = []\n",
    "number_pts_est = []\n",
    "for i, poly in enumerate(intersecting_polys):\n",
    "    wlayer_3DEP = ipyleaflet.WKTLayer(\n",
    "        wkt_string=poly[1].wkt, \n",
    "        style={\"color\": \"green\"})\n",
    "    \n",
    "    m.add_layer(wlayer_3DEP)\n",
    "    wlayer_3DEP_list.append(wlayer_3DEP)\n",
    "    usgs_3dep_datasets.append(poly[0])\n",
    "    \n",
    "    #estimate total points using ratio of area and point count\n",
    "    number_pts_est.append((int((AOI_EPSG3857.area/poly[2].area)*(poly[4]))))\n",
    "\n",
    "\n",
    "#make ipyleaflet layers from the AOI and add to map\n",
    "wlayer_user = ipyleaflet.WKTLayer(\n",
    "    wkt_string=AOI_GCS.boundary.wkt,\n",
    "    style={\"color\": \"blue\"}\n",
    ")\n",
    "\n",
    "AOI_EPSG3857_wtk = AOI_EPSG3857.wkt\n",
    "m.add_layer(wlayer_user)\n",
    "\n",
    "\n",
    "#sum the estimates of the number of points from each 3DEP dataset within the AOI\n",
    "num_pts_est = sum(number_pts_est)\n",
    "\n",
    "#Plot map and specify desired point cloud resolution using a widget\n",
    "user_resolution = widgets.RadioButtons(\n",
    "    options=[\n",
    "        (f'Full - All ~{int(math.ceil(num_pts_est/1e6)*1e6):,} points', 1.0),\n",
    "        (f'High - 2m resolution', 2.0),\n",
    "        (f'Mid  - 5m resolution', 5.0),\n",
    "        (f'Low  - 10m resolution', 10.0)\n",
    "    ],\n",
    "    layout={'width': 'max-content'},\n",
    "    disabled = False,\n",
    ")\n",
    "\n",
    "display(m)\n",
    "print(f'Your AOI at full resolution will include approximately {int(math.ceil(num_pts_est/1e6)*1e6):,} points. Select desired point cloud resolution.')\n",
    "widgets.VBox(\n",
    "    [user_resolution]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6cfefd",
   "metadata": {},
   "source": [
    "**Note**: Lidar point clouds can get *very* large, *very* fast. An AOI containing < 500,000,000 points *should* take less than ~20 minutes to access and store the data locally (or on Google Drive). An AOI containing < 50,000,000 points should take less than 5 minutes to do the same thing. Point clouds exceeding 1,000,000,000 points are accessible, even on local hardware, but they will take time to obtain and will likely require several to tens of gigabytes of storage on your local hard drive. Keep this in mind when deciding the AOI size and point cloud resolution above!\n",
    "\n",
    "The AOI bounding box, the relevant 3DEP dataset name(s), and the desired point cloud resolution are now defined. We can proceed with the API request to the AWS EPT bucket, processing, visualizing, and saving the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990899f",
   "metadata": {},
   "source": [
    "### Construct and Exectute PDAL Pipeline for Point Cloud Data\n",
    "The Point Data Abstraction Library (PDAL) is an open-source tool for translating and manipulating point cloud data. PDAL pipelines are useful ways of processing and manipulating point cloud data and creating derivative products. Pipelines comprise one or more stages that are read and executed in order on the point cloud dataset(s). \n",
    "\n",
    "The PDAL pipeline is constructed using the ```build_pdal_pipeline()``` function, and will construct the appropriate pipeline for the user's specifications (defined as function arguments). Executing this pipeline will make the API request, perform processing on the point cloud data (chosen by user) and provide the final result on the user's file system of Google Drive (Google Colab).  \n",
    "\n",
    "Paramaters (for more detailed descriptions of parameters, see <a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-6.3\">the function definitions</a>, above: <br>\n",
    "```user_wbd_poly_proj3857```: the user-defined HUC boundary<br>\n",
    "```usgs_3dep_datasets```: the intersecting 3DEP dataset names<br>\n",
    "```pointcloud_resolution```: point cloud resolution (1m, 2m, 5m, 10m)<br>\n",
    "```filterNoise```: remove the points of Class 7 (low noise) and Class 18 (high noise);<br>\n",
    "```reclassify```: remove USGS classes and run an SMRF to classify ground points only<br>\n",
    "```savePointCloud```: specify if point cloud data should be saved to local file system<br>\n",
    "```outCRS```: specify the coordinate reference system (CRS, in EPSG) of the output dataset.<br>\n",
    "```pc_outName```: name of point cloud on local file system<br>\n",
    "```pc_outType```: file type, las or laz (laszip compression). Options are 'las' or 'laz'<br>\n",
    "\n",
    "**Important Note 1: The ```user_wbd_poly_proj3857```, ```usgs_3dep_datasets```, and ```pointcloud_resolution``` arguments are already defined. These should not be modified.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555ea11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not modify user_wbd_poly_proj3857, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change pc_outname to descriptive name and pc_outType to 'las' or 'laz'.\n",
    "\n",
    "pointcloud_resolution = user_resolution.value\n",
    "pc_pipeline = build_pdal_pipeline(user_wbd_poly_proj3857, usgs_3dep_datasets, pointcloud_resolution, filterNoise = True,\n",
    "                                  reclassify = False, savePointCloud = True, outCRS = 3857,\n",
    "                                  pc_outName = 'wbd_test', pc_outType = 'laz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492ef96",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_pipeline = pdal.Pipeline(json.dumps(pc_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef24354",
   "metadata": {},
   "source": [
    "The cell below will execute the pc_pipeline object, which will make the API request, performing processing, and save the point cloud (if `savePointCloud == True`) at the specified location, name, and extension.\n",
    "\n",
    "The pipeline is executed in streaming mode, which significantly speeds up the process and cuts down on the required RAM. The `%%time` magic command will return the total computation time. The final output is the total number of points returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb758a8c",
   "metadata": {},
   "source": [
    "**Note**: If `reclassify = True`, the PDAL pipeline cannot be executed in streaming mode, because reclassifying requires all points to be present in memory. In this case the syntax for exectuting the pipeline without streaming is `pc_pipeline.execute()`. This will likely be slow and may not be possible for very large point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pc_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da107b41",
   "metadata": {},
   "source": [
    "If the user only desires point cloud data, they may stop here. Following is an overview on how a DSM and DTM may be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ed7a5",
   "metadata": {},
   "source": [
    "### Digital Elevation Model (DEM) Generation \n",
    "The following cells may be run to produce a DSM and DTM of the user-defined AOI. The DTM is produced using only points classified as 'Ground' (USGS Class 2), and therefore, represents the surface of the ground beneath any vegetation. Alternatively, a DSM uses all points to produce a representation of the surface including vegetation and other structures. \n",
    "\n",
    "The DTM/DSM is produced using an analogous approach to point cloud request and processsing. Namely, a PDAL pipeline is constructed using the function `make_DEM_pipeline()`. This function first constructs a pipeline using `build_pdal_pipeline()` to create stages related to point cloud access and processing. The function then appends PDAL stages specific to the creation of gridded products that fit the specification of the user (defined as function arguments), namely dem resolution and dem type.\n",
    "\n",
    "Paramaters (for more detailed descriptions of parameters, see <a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-6.3\">the function definitions</a>, above:<br>\n",
    "`user_wbd_poly_proj3857`: the user-defined area of interest (AOI)<br>\n",
    "`usgs_3dep_dataset`: the corresponding 3DEP dataset name<br>\n",
    "`pointcloud_resolution`: point cloud resolution (1m, 2m, 5m, 10m)<br>\n",
    "`dem_generation`: grid size for dem product (specified in meters)<br>\n",
    "`filterNoise`: remove the points of Class 7 (noise); optional<br>\n",
    "`reclassify`: remove USGS classes and run an SMRF to classify ground points only<br>\n",
    "`savePointCloud`: specify if point cloud data should be saved to local file system<br>\n",
    "`pc_outName`: name of point cloud on local file system<br>\n",
    "`pc_outType`: file type, las or laz (laszip compression). Options are 'las' or 'laz'<br>\n",
    "`demType`: specifies to create digital surface model (DSM) or digital terrain model (DTM)<br>\n",
    "`gridMethod`: gridding method to use; options: (min, mean, max, idw)<br>\n",
    "`dem_outName`: name of dem on local file system <br>\n",
    "`dem_outExt`: extension of file on local file system (must correspond to what is chosen for ```driver```<br>\n",
    "`driver`: gdal code of the driver (default is \"GTiff\"; other options can be found at https://gdal.org/drivers/raster/index.html<br>\n",
    "\n",
    "**Important note 1: The `make_DEM_pipeline()` function is used for the making of both DSM and DTM products. The type of DEM (DSM/DTM) is specified in the `demType` argument (e.g., `demType = 'dsm'`)**\n",
    "\n",
    "**Important Note 2: The `user_wbd_poly_proj3857`, `usgs_3dep_datasets`, and `pointcloud_resolution` arguments are already defined after running the above cells. These should not be modified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564000a",
   "metadata": {},
   "source": [
    "### Make Digital Surface Model (DSM)\n",
    "The following cells will produce a Digital Surface Model (DSM) using all of the lidar returns in the point cloud.\n",
    "Do not modify the `AOI_EPSG3857_wtk`, `usgs_3dep_datasets`, or `pointcloud_resolution` arguments. Specify the desired dsm resolution (in meters), the appropriate point cloud processing steps, and the file names/extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23c3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not modify user_wbd_poly_proj3857, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change dem_outName to descriptive name; dem_outExt can be any extension supported by gdal.\n",
    "\n",
    "pointcloud_resolution = user_resolution.value\n",
    "dem_resolution = 2.0\n",
    "dem_pipeline = make_DEM_pipeline(user_wbd_poly_proj3857, usgs_3dep_datasets, pointcloud_resolution, dem_resolution,\n",
    "                                 filterNoise = True, reclassify = False,  savePointCloud = False, outCRS = 3857,\n",
    "                                 pc_outName = 'wbd_test', pc_outType = 'laz', demType = 'dsm',\n",
    "                                 gridMethod='idw', dem_outName = 'test_dsm_wbd', dem_outExt = 'tif', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39029939",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed for making the DSM. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_pipeline = pdal.Pipeline(json.dumps(dem_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5491c",
   "metadata": {},
   "source": [
    "The cell below will execute the dsm_pipeline object, which will make the API request, performing processing, and save the point cloud (if `savePointCloud == True`) and create and save the DSM at the specified location, name, and extension. \n",
    "\n",
    "The pipeline is executed in streaming mode, which significantly speeds up the process and cuts down on the required RAM. The `%%time` magic command will return the total computation time. The final output is the total number of points returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749cf45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dem_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1dcee",
   "metadata": {},
   "source": [
    "Below, the same process is outlined for the making of at DTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de2d50",
   "metadata": {},
   "source": [
    "### Make Digital Terrain Model (DTM)\n",
    "The following cells will produce a Digital Terrain Model (DTM), also called a 'bare earth model' using lidar returns classified as 'ground' (USGS Class 2). Do not modify the `user_wbd_poly_proj3857`, `usgs_3dep_datasets`, or `pointcloud_resolution` arguments. Specify the desired dtm resolution (in meters), the appropriate point cloud processing steps, and the file names/extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06655269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not modify user_wbd_poly_proj3857, usgs_3dep_datasets, or pointcloud_resolution\n",
    "# Modify the optional arguments to fit user need.\n",
    "# Change outCRS to EPSG code of desired coordinate reference system (Default is EPSG:3857 - Web Mercator Projection)\n",
    "# Change dem_outname to descriptive name and change dem_outExt and driver to desired file type.\n",
    "\n",
    "pointcloud_resolution = user_resolution.value\n",
    "dem_resolution = 2.0\n",
    "dem_pipeline = make_DEM_pipeline(user_wbd_poly_proj3857, usgs_3dep_datasets, pointcloud_resolution, dem_resolution,\n",
    "                                 filterNoise = True, reclassify = False, savePointCloud = False, outCRS = 3857,\n",
    "                                 pc_outName = 'wbd_test', pc_outType = 'laz', demType = 'dtm', \n",
    "                                 gridMethod = 'idw', dem_outName = 'test_dtm_wbd', dem_outExt = 'tif', driver = \"GTiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21047172",
   "metadata": {},
   "source": [
    "The PDAL pipeline is now constructed for making the DTM. Running the the PDAL Python bindings function ```pdal.Pipeline()``` creates the pdal.Pipeline object from a json-ized version of the pointcloud pipeline we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_pipeline = pdal.Pipeline(json.dumps(dem_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb6bac",
   "metadata": {},
   "source": [
    "The cell below will execute the dtm_pipeline object, which will make the API request, performing processing, and save the point cloud (if `savePointCloud == True`) and create and save the DTM at the specified location, name, and extension. \n",
    "\n",
    "The pipeline is executed in streaming mode, which significantly speeds up the process and cuts down on the required RAM. The `%%time` magic command will return the total computation time. The final output is the total number of points returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59ee07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dem_pipeline.execute_streaming(chunk_size=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8990d5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualize the DEM (DSM/DTM)\n",
    "We can now visualize the DSM or DTM products in the Jupyter Notebook. We use the <a href=\"https://corteva.github.io/rioxarray/stable/\"> rioxarray </a> and <a href=\"https://matplotlib.org/stable/users/index.html\"> matplotlib </a> Python libraries for simple plotting. We import `rioxarray` and `matplotlib.pyplot` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5664ce2",
   "metadata": {},
   "source": [
    "Now we must define the file name we would like to plot. This could be a file path (e.g., `/path/to/my/dtm/dtm.tif`). Then we open the dtm as an `xarray` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_name = 'test_dtm_wbd.tif'\n",
    "dtm = rioxarray.open_rasterio(dtm_name, masked=True) # /path/to/your/dtm/dtm.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3821666",
   "metadata": {},
   "source": [
    "Now we plot the DTM. By default, we use the 'viridis' colorbar to plot the bare earth elevation. Other colormaps can be used, and more information about available colormaps can be found here: https://matplotlib.org/stable/tutorials/colors/colormaps.html ).\n",
    "\n",
    "Using the argument `robust=True` stretches the colors between the 2nd and 98th percentiles of our elevation data. This is just for visualization - the original values are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "dtm.plot(cmap=\"viridis\", robust=True)\n",
    "plt.title(\"Digital Terrain Model (DTM) in Meters\")\n",
    "plt.ticklabel_format(style=\"plain\")\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca356a",
   "metadata": {},
   "source": [
    "Pretty cool, right? But what if we would like to look at a statistical distribution of the elevation in this region? We can plot a simple histogram, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.style.use('seaborn') # prettier plot\n",
    "dtm.plot.hist(bins=100)\n",
    "plt.title(\"Distribution of Elevation in Meters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c4c69",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This Jupyter Notebook is designed to make accessing, processing, visualizing, and creating derivative products with 3DEP more straightforward. Specifically, this notebook provides a workflow for programmatically accessing the 3DEP lidar point cloud data using an API request to the cloud-hosted AWS S3 bucket, performing basic processing steps on the point cloud data, and producing digital elevation models (DTMs/DSMs) user-defined USGS 12- and 14-digit hydrologic units.\n",
    "\n",
    "Feedback regarding the workflow, specific use cases, and any computational testing with the notebook are welcomed and encouraged! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf65c16",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "All OpenTopography USGS 3DEP scientific workflows in this collection:<br>\n",
    "\n",
    "1. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/01_3DEP_Generate_DEM_User_AOI.ipynb) <br>\n",
    "\n",
    "2. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS 7.5â€™ Quadrangles](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/02_3DEP_Generate_DEM_USGS_7.5_Quadrangles.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/02_3DEP_Generate_DEM_USGS_7.5_Quadrangles.ipynb) <br>\n",
    "\n",
    "3. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for USGS Hydrologic Units](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/03_3DEP_Generate_DEM_USGS_HUCs.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/03_3DEP_Generate_DEM_USGS_HUCs.ipynb) <br>\n",
    "\n",
    "4. [Generate and visualize DEMs (DTM and DSM) from USGS 3D Elevation Program (3DEP) lidar data for user-defined corridors](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/04_3DEP_Generate_DEM_Corridors.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/04_3DEP_Generate_DEM_Corridors.ipynb) <br>\n",
    "\n",
    "5. [Generate Canopy Height Model (CHM) using USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/05_3DEP_Generate_Canopy_Height_Models_User_AOI.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/05_3DEP_Generate_Canopy_Height_Models_User_AOI.ipynb) <br>\n",
    "\n",
    "6. [Topographic Differencing using USGS 3D Elevation Program (3DEP) lidar data for user-defined area of interest](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/06_3DEP_Topographic_Differencing.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/06_3DEP_Topographic_Differencing.ipynb) <br>\n",
    "\n",
    "7. [Generate colorized (RGB) point clouds using USGS 3D Elevation Program (3DEP) lidar data and National Agriculture Imagery Program (NAIP) Imagery](https://github.com/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/07_3DEP_Generate_Colorized_PointClouds.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cmspeed/OT_3DEP_Workflows/blob/main/notebooks/07_3DEP_Generate_Colorized_PointClouds.ipynb)\n",
    "    \n",
    "### Additional Resources\n",
    "\n",
    "- Access USGS 3DEP via the <a href=\"https://portal.opentopography.org/datasets\">OpenTopography</a> portal (Currently restricted to academics).\n",
    "\n",
    "- The USGS 3DEP Lidar Point Cloud Data are accessible in Entwine Point Tile (EPT) format from this <a href=\"https://registry.opendata.aws/usgs-lidar/\">Amazon Web Services S3 Bucket</a>. <br>\n",
    "\n",
    "- The USGS hydrologic unit boundaries are accessed via the <a href=\"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer\">USGS Watershed Dataset Map Service</a>. <br>\n",
    "\n",
    "- The USGS 7.5' quadrangle boundaries are accessed via the <a href=\"https://carto.nationalmap.gov/arcgis/rest/services/map_indices/MapServer\"> USGS Map Indicies Service</a>. <br>\n",
    "\n",
    "- Documentation for open-source Python libararies used by these workflows include <a href=\"https://pdal.dev/en/latest/\">PDAL</a> and <a href=\"https://gdal.org/\">GDAL</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
